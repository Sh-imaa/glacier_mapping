{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set some of the parameters used throughout this notebook. We'll be focused on a single geographic split -- change the split ID to see another run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "geo_dir = pathlib.Path(\"/datadrive/glaciers/expers/geographic\")\n",
    "split_dir = geo_dir / \"splits/2\"\n",
    "root_dir = pathlib.Path(\"/home/kris/glacier_mapping/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in the final trained model from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from glacier_mapping.models.frame import Framework\n",
    "from glacier_mapping.models.metrics import diceloss\n",
    "from glacier_mapping.data.data import fetch_loaders\n",
    "from glacier_mapping.train import validate\n",
    "from addict import Dict\n",
    "\n",
    "conf = yaml.safe_load(open(root_dir / \"conf/train.yaml\", \"r\"))\n",
    "conf = Dict(conf)\n",
    "\n",
    "frame = Framework(\n",
    "    model_opts=conf.model_opts,\n",
    "    optimizer_opts=conf.optim_opts,\n",
    "    reg_opts=conf.reg_opts,\n",
    "    loss_fn=diceloss(act=torch.nn.Softmax(dim=1), w=[1, 1, 0])\n",
    ")\n",
    "\n",
    "state = torch.load(split_dir / \"runs/demo/models/model_final.pt\")\n",
    "frame.model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point,Â we can compute metrics across all the batches in the train, development, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glacier_mapping.models.metrics import tp_fp_fn\n",
    "\n",
    "train_loaders = fetch_loaders(split_dir, 16)\n",
    "test_loaders = fetch_loaders(split_dir, 16, dev_folder = \"test\")\n",
    "\n",
    "\n",
    "def error_row(rates, ix=1, split=\"train\"):\n",
    "    tp, fp, fn = rates\n",
    "    tp, fp, fn = tp.cpu().numpy(), fp.cpu().numpy(), fn.cpu().numpy()\n",
    "    return {\n",
    "        \"split\": split,\n",
    "        \"tp\": tp[ix],\n",
    "        \"fp\": fp[ix],\n",
    "        \"fn\": fn[ix],\n",
    "        \"IoU\": tp[ix] / (tp[ix] + fp[ix] + fn[ix])\n",
    "    }\n",
    "\n",
    "\n",
    "def aggregate_rates(metrics, loader, split=\"train\"):\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    for x, y in loader:\n",
    "        y_hat = frame.infer(x.to(device))\n",
    "        rates = tp_fp_fn(y_hat > 0.4, y.to(device))\n",
    "        metrics[\"debris\"].append(error_row(rates, 1, split))\n",
    "        metrics[\"clean_ice\"].append(error_row(rates, 0, split))\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "metrics = {\"debris\": [], \"clean_ice\": []}\n",
    "metrics = aggregate_rates(metrics, train_loaders[\"train\"], \"train\")\n",
    "metrics = aggregate_rates(metrics, train_loaders[\"val\"], \"val\")\n",
    "metrics = aggregate_rates(metrics, test_loaders[\"val\"], \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get the overall IoU, by summing the true and false positive pixels over all terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "debris = pd.DataFrame(metrics[\"debris\"])\n",
    "clean_ice = pd.DataFrame(metrics[\"clean_ice\"])\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    debris_ = debris[debris[\"split\"] == split]\n",
    "    clean_ = clean_ice[clean_ice[\"split\"] == split]\n",
    "    print(debris_[\"tp\"].sum() / (debris_[\"tp\"].sum() + debris_[\"fp\"].sum() + debris_[\"fn\"].sum()))\n",
    "    print(clean_[\"tp\"].sum() / (clean_[\"tp\"].sum() + clean_[\"fp\"].sum() + clean_[\"fn\"].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd be useful to have some context about which slices get the highest / lowest errors. Do you get worse performance the further away you get from the training area? To answer this, we'll want to look at the slice metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "slice_meta = gpd.read_file(\"/datadrive/glaciers/processed_exper/slices/slices.geojson\", crs=\"EPSG:3857\")\n",
    "slice_meta = slice_meta.to_crs(\"EPSG:4326\")\n",
    "slice_meta[\"img_basename\"] = [pathlib.Path(s).stem for s in slice_meta[\"img_slice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_meta[slice_meta[\"img_basename\"] == \"slice_11_img_081\"]\n",
    "\n",
    "paths = {}\n",
    "paths[\"train\"] = train_loaders[\"train\"].dataset.img_files\n",
    "paths[\"val\"] = train_loaders[\"val\"].dataset.img_files\n",
    "paths[\"test\"] = test_loaders[\"val\"].dataset.img_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
